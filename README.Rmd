---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# peruse

<!-- badges: start -->
<!-- badges: end -->

The {peruse} package is aimed at making it easier to generate irregular
sequences that are difficult to generate with existing tools. The heart
of {peruse} is the `S3` class `Iterator`. An `Iterator` allows the
user to write an arbitrary R expression that returns the next
element of a sequence of R objects. It then saves the state of the `Iterator`, meaning the next time `yield_next` is called, the subsequent element of the sequence will be returned.

The package also provides a simple, tidy API for set building, allowing the user to generate a set consisting of the elements of a vector that meet specific criteria. This can either return a vector consisting of all the chosen elements or it can return an `Iterator` that lazily generates the chosen elements.

Finally, {peruse} also provides a new data structure stores a `data.frame` as an object with reference semantics and `O(1)` access to columns. This is useful when iterating over `data.frame`s with many columns, because the object is modified in place, rather than making a shallow copy on every iteration.

## Installation

You can install the released version of peruse from [CRAN](https://CRAN.R-project.org) with:

``` r
install.packages("peruse")
```

And the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("jacgoldsm/peruse")
```
## Example
### Collatz Sequence

A Collatz sequence is a particular sequence of natural numbers that mathematicians think always reaches `1` at some point, no matter the starting point. We can't prove that one way or the other, but we can create an `Iterator` that lazily generates a Collatz sequence until it reaches `1`:

```{r example}
library(peruse)
expr <- "if (n %% 2 == 0) n <- n / 2 else n <- n*3 + 1"
  
# Collatz generator starting at 50
collatz <- Iterator(result = expr,
                    initial = c(n = 50),
                    yield = n)

i <- 0
while (i != 1L) {
  i <- yield_next(collatz)
  cat(paste0(i, "\n"))
}
```
### Random Walk with Drift
Random Walks, with or without drift, are one of the most commonly used type of stochastic processes. How can we simulate one with \code{peruse}?

```{r}
set.seed(1)
expr <- 'n <- n + sample(c(-1L, 1L), size = 1L, prob = c(0.25, 0.75))'
rwd <- Iterator(result = expr,
                initial = c(n = 0),
                yield = n)


Value <- integer()
reach <- 0L
while (reach != 50L & reach != -50L) {
  reach <- yield_next(rwd)
  Value <- c(Value, reach)
}

plot(Value, main = "The Value of the Iterator after a Given Number of Iterations")
```

Here, we can see that `seq` gets to `50` after about `100` iterations when it is weighted `3:1` odds in favor of adding `1` over adding `-1` to the prior value.

### Primes
How about generating all the prime numbers between `1` and `100`? We can easily do that with the set-builder API:
```{r}
cat(2:100 %>% that_for_all(range(2, .x)) %>% we_have(~.x %% .y != 0))
```
But how about if we want to generate the first $100$ prime numbers? We don't know the range of values this should fall in (well, mathematicians do), so we can use laziness to our advantage:
```{r}
primes <- 2:10000 %>%
            that_for_all(range(2, .x)) %>% 
            we_have(~.x %% .y != 0, "Iterator")

sequence <- c()
while (length(sequence) <= 100) {
  sequence <- c(sequence, yield_next(primes))
}

cat(sequence)
```
### hash_df
When dealing with text data, it is common to run models on a very wide data set, with thousands of variables representing each token. Doing transformations on a wide data set can be very expensive, as every time a column is transformed, a shallow copy is made of the `data.frame`.

Here is an example of a wide data set representing the reviews of a great variety of wines. Each column represents a token, and the values represent the raw count of the number of times the word appears in a given review.
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidytext)
library(tidyverse)

desc <- read_csv("/home/storeddocumentsonline/data/winemag-data_first150k.csv")
desc <- desc[1:20000,]
tidy_descriptions <- desc %>%
  mutate(ident = as.character(1:nrow(desc))) %>%
  unnest_tokens(word, description) %>%
  count(ident, word) %>%
  bind_tf_idf(word, ident, n)

rm(desc)

wide_descriptions <- tidy_descriptions %>%
  select(ident, word, n) %>%
  pivot_wider(names_from = word, 
              names_prefix = "word_",
              values_from = n, 
              values_fill = 0) %>%
  as_tibble()

rm(tidy_descriptions)
cat(paste0("Dimensions: ", nrow(wide_descriptions), " by ", ncol(wide_descriptions)))
wide_descriptions[1:5, 1:5]
```
What if we want to normalize all these columns by the absolute frequency of the word in the data set? Normally, it would take a very long time to iterate over all $15,578$ columns. However, it is very fast with a `hash_df`:
```{r}
hash_frequencies <- hash_df$new(wide_descriptions)

transform <- function(x) {
  if (is.numeric(x)) x / sum(x) else x
}
hash_frequencies$data <- lapply(hash_frequencies$data,
                            transform)

(hash_frequencies$return_df())[1:5, 1:5]
```
(Note that the column order gets scrambled, as environments have no element order).

Even easier, we can use implementations of `dplyr`-esque `mutate_*` functions on the objects:

```{r}
hash_frequencies <- hash_df$new(wide_descriptions)

hash_frequencies$mutate_if(is.numeric, ~.x / sum(.x))
```

Any such task is quick and easy with the `hash_df`!

## Functions

### Iterators:
- as_Iterator()
- is_Iterator()
- Iterator()
- yield_next()

### Sets
- that_for_all()
- that_for_any()
- we_have()

### hash_df
- $bind()
- $unbind()
- $return_df()
- $print()
- $new()
- $View()
- $mutate()
- $mutate_if()
- $mutate_all()
- $select()
- $select_if()
- $select_at()
- $clone()

